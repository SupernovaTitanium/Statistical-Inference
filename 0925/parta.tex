\def\draft{0}
\documentclass[11pt]{article}
\usepackage{amsfonts, fullpage, rotating, amssymb}
\usepackage{color,amsmath}
\usepackage{IEEEtrantools}
\input{macros}
\pagestyle{plain}
\newcommand{\scribe}{Wei-Chang Lee}
\newcommand{\lecnum}{4}
\newcommand{\lecdate}{September 27, 2015}

%\parskip=1.5mm
%\parindent=0mm

\begin{document}
	
	\begin{center}
		\renewcommand{\arraystretch}{2}
		\begin{bfseries}
			\begin{tabular}{|c|}
				\hline
				Statistical Inference I \hfill Prof. Chin-Tsang Chiang\\
				\hspace{15em} {\large Lecture Notes \lecnum} \hspace{15em}\ \\
				\lecdate \hfill Scribe: \scribe\\
				\hline
			\end{tabular}
			\renewcommand{\arraystretch}{1}
		\end{bfseries}
	\end{center}
	
Today we're going to talk about method of counting to be able to study probability assignments on finite space, we will then give definition about conditional probability on discrete case and intuition about continuous case. After knowing how to calculate conditional probabilities, we can then use Bayes' theorem to connect experimental study to observation study which is called the odds ratio. At the end of the class, Prof.Chiang introduce the concept "independence" and how to interpret it beyond geometric views in a more statistical way.
\section{Counting Methods}
  There are two kinds of interpretation of probability measure. 
\begin{enumerate}
\item[1]
  The first kind of view is based on the "frequency of occurrence". They do random experiments many times to study  how many times event which is interested in would take place. Then, they assign the limit ratio  as the probability of certain events. The intuition of this interpretation is that we believe the underlying parameter is invariant so after many repeated experiment the random effect can be cancelled out and the real instinct reveal. 
\item[2]
  The second kind of view is just based on the "subjective belief of interpreter" in the chance of an event occurring.  We can just give probability under our faith before any experiment has been done and then do experiment to study whether our prior assumption fit the data or not and then modify it. This procedure can be done by Bayes' theorem. 
\end{enumerate}
\paragraph{}
  The counting problem is sometimes sophisticated and along with many restrictions. The way to solve such problem is to break them into series of simple tasks and apply rules to combine it back. 
\begin{theorem}[Fundamental Theorem of Counting]
	Suppose a job consists of k separate tasks, the $i$th of which can be done in $n_i$ ways, then entire job can be done in $\prod_{i=1}^k n_i$ ways.
\end{theorem}
\begin{intuition}[Classification of counting methods]
	The proof of Fundamental Theorem of Counting is quite trivial. Sometimes it is better to think of task as partition criteria such as love or hate, different gender or income level then we just construct a sample space $\Omega=\Omega_1\times\Omega_2...\Omega_k$ which is the cartesian product of k criteria we interested in then we can apply the theorem to calculate there are how many possible outcomes.\\
In reality, we may face situation such as replacement and unordered. Replacement means that $n_i=n_{i+1}$ and unordered means that we can perform k separate tasks arbitrary without certain order. In such case we have to carefully apply or modify the fundamental counting theorem. 	
\end{intuition}
Consider number of possible arrangements of size r drawing from n different subjects. We can divide the case into four categories.
\begin{enumerate}
	\item Without replacement and ordered: $n*(n-1)*(n-2)...(n-r+1)=\prod_{i=1}^r(n-i+1)=\frac{n!}{(n-r)!}$
	\item With replacement and ordered:$n*n*n...n=\prod_{i=1}^r n=n^r$
	\item Without replacement and unordered: $\frac{\prod_{i=1}^r(n-i+1)}{r!}=\frac{n!}{(n-r)!(r!)}=\binom{n}{r}$ 
	\item With replacement and unordered: $\binom{n+r-1}{r}$
\end{enumerate}
\begin{proof}
Case 1 and case 2 is quite simple just applying the Fundamental Theorem of Counting. For case 3, r different objects can be permuted in $r*(r-1)*(r-2)...1=r!$ ways but they represent the same arrangement in unordered situation. So we have case 3 equals case 1 divided by r!. Case 4 is the most difficult, we may simply view it as case 2 divided by r! but it will underestimate the possible arrangements since r objects with some of them are of same kind do not construct r! different permutations. A clever way to solve case 4 is to think of r as numbers of coins and place it arbitrary but all into n different box. A coin in $i$th box means in our ultimate arrangements we have one $i$th objects. Consider a small case as n=3 and r=3. Then the following figure is the realization of picking two $2$th object and one $3$th object. And all possible realization cab be expressed as all possible arrangements of 2 $|$ and 3 $O$ which is $\binom{3-1+3}{3}$. 
$$\underline{\quad}^1 |\underline{OO}^2 |\underline{O}^3$$ So for case 4, the answer is $\binom{n-1+r}{r}$.
\end{proof}
\paragraph{}
The counting techniques are useful when the sample space is finite and every possible outcomes in S are equally likely. The probability of certain event can be calculated by the number of outcomes in that event times the probability of each outcome from the countably addictive axiom.
\begin{theorem}[Enumerating outcomes]
Let $\Omega=\{w_1,w_2...w_n\}$ with $P(\{w_i\})=\frac{1}{n}$ $\forall i$ then $P(A)=\sum_{\{w_i\}\in A}P(\{w_i\})=\sum_{\{w_i\}\in A}\frac{1}{n}=\frac{\sharp(A)}{\sharp(\Omega)}$ $\forall A$.
\end{theorem}
\begin{remark}
This is also the classical definition of probability from Pierre-Simon Laplace.
\end{remark} 
\section{Conditional probability}
In reality, we may need to study something like if she is a girl, what is the probability that she wants to get married. Studying these kinds of probabilities under certain situation or restrictions of sample space needs the definition of conditional probability.
\begin{theorem}
Let P(.) be a probability measure on $\sigma$-algebra $F$, A and B be events in $F$ and P(B)$>$0 then, $P(.|B)$, the conditional probability of A given B is denoted by  
$$P(A|B)=\frac{P(A\cap B)}{P(B)}$$
\end{theorem}
First we need to verify $P(.|B)$ is a probability measure.
\begin{enumerate}
\item[1] $P(A|B)\geq 0$  $\forall A\in F$ : Since $P(B)>0$ and $P(A\cap B)\geq 0$ because P(.) is a probability measure, then the ratio  $\frac{P(A\cap B)}{P(B)}\geq 0$ $\forall A\in F$.
\item[2] $P(\Omega|B)=1$ : $\frac{P(\Omega\cap B)}{P(B)}=\frac{P(B)}{P(B)}=1$.
\item[3] $A_1,A_2...A_n...\in F$, $A_i\cap A_j=\phi$ then $P(\cup_{i=1}^\infty A_i|B)=\sum_{i=1}^{\infty}P(A_i|B)$ : First notice that $A_1\cap B,A_2\cap B...A_n\cap B...$ are all disjoint then $P(\cup_{i=1}^\infty A_i|B)=\frac{P(\cup_{i=1}^\infty A_i\cap B)}{P(B)}=\frac{P(\cup_{i=1}^\infty (A_i\cap B))}{P(B)}=\frac{\sum_{i=1}^{\infty}P(A_i\cap B)}{P(B)}=\sum_{i=1}^{\infty}P(A_i|B)$.
\end{enumerate}
$P(.|B)$ is indeed a probability measure, but what is its statistical meaning?
\begin{example} 
Consider a case that we need to study how accurate our new AIDS test is, then the sample space is partitioned by two criteria. The test result is positive/negative and the subject has AIDS or not. We have  $\Omega:\{(Test,Disease):T\in\{+,-\},D\in\{+,-\}\}=\Omega_T\times\Omega_D$, then
\begin{align*} 
P(D_+|T_+)=\frac{P(D_+\cap T_+)}{P(T_+)}=\frac{P(\{(D_+,T_-),(D_+,T_+)\}\cap\{(D_+,T_+),(D_-,T_+)\})}{P(\{(D_+,T_+),(D_-,T_+)\})}
\end{align*} 
So the conditional probability is actually the study of how sub-sample space $\Omega_T,\Omega_D$ will affect each other on $\Omega$ i.e. what is the relation between each partition criteria, we may not simply think conditional probability in geometric view.   
\end{example}
\begin{intuition}[Conditional Probability]
Generally speaking, $P(B)>0$ cannot always be satisfied since we may be interested in continuous data such as heights and weights. We face division by zero when study problem like given father is 1.75 tall what is the probability the son is 1.80 tall. We have $P(\text{father is 1.75})=0$ and above definition does not work any more. A natural way to save it is to think of B as a small neighbourhood $(B-\frac{\bigtriangleup}{2},B+\frac{\bigtriangleup}{2})$ so we have $$P(A|B)=\frac{\lim_{\bigtriangleup\rightarrow 0} P(A\cap (B-\frac{\bigtriangleup}{2},B+\frac{\bigtriangleup}{2}) )/\bigtriangleup}{\lim_{\bigtriangleup\rightarrow 0}P(B-\frac{\bigtriangleup}{2},B+\frac{\bigtriangleup}{2})/\bigtriangleup}$$  
Noting that both the numerator and denominator are of change rate form so $P(A|B)$ can be think of as the intensity of the ratio of change rate.\\
In the previous test-disease example, we can not do medical behavior  relying on uncertified test for moral issue. We may only do the reverse direction test. Do the test to patients who have the disease and what is the test result respond to it. So we have only $P(T_+|D_+)$ and also the prevalence of the disease $P(D_+)$. How can we know  $P(D_+|T_+)$? This is where we need the Bayes' theorem.  
\end{intuition}
\section{Bayes' Theorem}
\end{document}




