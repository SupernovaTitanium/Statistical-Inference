\documentclass[../Probability_Theory.tex]{subfiles}
\begin{document}
	\begin{center}
		\renewcommand{\arraystretch}{2}
		\begin{bfseries}
			\begin{tabular}{|c|}
				\hline
				Statistical Inference I \hfill Prof. Chin-Tsang Chiang\\
				\hspace{15em} {\large Lecture Notes 5} \hspace{15em}\ \\
				\lecdate \hfill Scribe: \scribe\\
				\hline
			\end{tabular}
			\renewcommand{\arraystretch}{1}
		\end{bfseries}
	\end{center}

After explaining the concept of independence, today we can finally introduce the Second Borel-Cantelli Lemma. A lemma that can lately be used to prove one of the most important result in probability theory, the strong law of large numbers. Also, we talk about another fundamental concept in probability, random variable, which let us change our views from set to the real field.   

\section{The Second Borel-Cantelli Lemma}
Also called the reverse Borel-Cantelli Lemma.
\begin{theorem}
Let $A_1,A_2,A_3...A_n...\in \mathcal{A}$ be mutually independent. If $\sum_{i=1}^{\infty}P(A_i)=\infty$, then	$$P(A_{n.i.o}) = 1$$
	, where $A_{n.i.o.} = \limsup_{n\rightarrow\infty}A_n.$
\end{theorem}
\begin{proof}
First Notice that since $\{A_n\}$ are independent, so $P(\bigcap_{k=n}^{n+j}A_k^c)=\prod_{k=n}^{n+j}(1-P(A_k))$ and $1-x\leq e^{-x}$ $\forall x\geq 0$ then $$P(\bigcap_{k=n}^{n+j}A_k^c)\leq e^{-\sum_{k=n}^{n+j}P(A_k)} \ \forall n$$
And $\sum_{i=1}^{\infty}P(A_i)=\infty$, one has 
$$\lim_{j\rightarrow\infty}P(\bigcap_{k=n}^{n+j}A_k^c)\leq \lim_{j\rightarrow\infty}e^{-\sum_{k=n}^{n+j}P(A_k)}=0$$ From Boole's inequality, it follows that $$P(\bigcup_{n=1}^\infty\bigcap_{k=n}^\infty A_k^c)\leq \sum_{n=1}^\infty P(\bigcap_{k=n}^\infty A_k^c)=0$$Then by De Morgan's rule we have,
$$P(\limsup_{n\rightarrow\infty}A_n)=P(\bigcap_{n=1}^\infty\bigcup_{k=n}^\infty A_k)=1-P(\bigcup_{n=1}^\infty\bigcap_{k=n}^\infty A_k^c)=1$$
\end{proof}
So when $\{A_n\}$ are mutually independent, combining both Borel-Cantelli lemma, we have a zero-one law: Borel's zero-one law
$$P(A_{n.i.o.})\text{ is either 0 or 1 with respect to} \sum_{i=1}^{\infty}P(A_i) \text{ converged or not}. 
$$
This give us a peek of Kolmogorov's zero-one law.
\begin{theorem}
Suppose a probability space $(\Omega,\mathcal{A},P)$and $A_n$ be a sequence of mutually independent $\sigma$-algebras contained in $\mathcal{A}$  then the tail events $F\in \bigcap_{n=1}^{\infty}G_n$ where $G_n=\sigma(\bigcup_{k=n}^\infty A_k)$ is either zero or one i.e. $P(F)=1$ or $P(F)=0$.
\end{theorem}
\begin{remark}
A naive way to understand the theorem is to consider if you have a sequence of independent random variables and an event that is invariant if you ignore finitely many of the variables, then the probability of that event is either 0 or 1 .
\end{remark}
\begin{remark}
Independent assumption is really important in the Borel's zero one law. If a event E with $0<P(E)<1$ and let a sequence of events $\{A_n\}$ where each $A_i=E$ then it is clear  $\{A_n\}$ are not independent at all and $P(A_{n.i.o.})=P(E)$ which violates the zero one law.
\end{remark}
\begin{intuition}[Borel Cantelli Lemma]
The Borel Cantelli Lemma enable us to construct probability in infinite many time/games from probability in sequence of events. If we toss a coin or play a independent game infinite many times, and let $A_n$ be events in $n$th toss/round. We can first study a pattern of coin or strategy in games has how much chance to win in one toss/round then use Borel Cantelli Lemma to extend it to infinite case that certain patterns will definitely happen or our strategy can finally win. An intersting example is given as below.
\end{intuition}
\begin{example}[Infinite Monkey Theorem]
Suppose a monkey has a typewriter, he types any alphabet at random. If he never stops typing, then he can almost surely type any Shakespeare's sonnet.
\end{example}
\begin{solve}
Consider the alphabets sequence the monkey types, we can divide this infinitely long string to infinitely many chunks, each chunk has length k where k equals to the number of alphabets in given Shakespeare's sonnet. Let $\{E_n\}$ denotes the monkey successfully types the sonnet in the $n$th chunk, then we have 
$$P(E_n)=\frac{1}{26^k}>0 \text{ so }\sum_{n=1}^\infty P(E_n)=\infty$$
And chunks are independent from one another. Using the Second Borel Cantelli Lemma, we get
$$P(E_{n.i.o.})=1$$ 
There are infinite many chunks, the monkey almost surely types Shakespeare's sonnet.
\end{solve}
\begin{remark}
In reality, the monkey can not fulfil this since not only did he has certain behavior pattern but also reality is a very big finite number. If the universe is filled with monkeys and they keep typing, the chance to type Hamlet is less than $\frac{1}{10^{183800}}$. The gap between infinite and finite is bigger than every big finite number we can guess.  
\end{remark}
\begin{example}[Strong Law of large number]We neglect the proof but argue how it can be used in proving it.We want to show that $$P(\lim_{n\rightarrow\infty}\frac{S_n}{n}=\mu)=P(\Omega_0)=1$$ 
Where $S_n=X_1+X_2...+X_n$ and $E(X_i)=\mu$ then it is equivalent to there exist $\Omega_0\in \mathcal{A}$ for every $w\in \Omega_0$ it holds that $$\lim_{n\rightarrow\infty}|\frac{S_n}{n}-\mu|=0$$ 
It suffices to show for any $\epsilon>0$, $|\frac{S_n}{n}-\mu|>\epsilon$ can occur only a finite number of times. And it can be done by Borel-Cantelli Lemma.
\end{example}
\section{Random Variables}
Before giving formal definition of what is a random variable. Let use first introduce two important terms, Borel sets and $\Sigma$-measurable function.
\begin{definition}[Borel $\sigma$-algebra]
Let S be a topological space then $\mathcal{B}(S)$, the Borel $\sigma$-algebra on S, is the $\sigma$-algebra generated by the family of open subsets of S. $$B(S):=\sigma(\text{open sets})$$
And $$\mathcal{B}:=B(R)$$
\end{definition}
Every subsets of R we meet in everyday use is an element of $\mathcal{B}$; but elements of $\mathcal{B}$ can be quite complicated. An easy way to understand $\mathcal{B}$ is by the $\pi$ system (closed under finite intersection) $\xi$, $$\xi=\pi(R):=\{(-\infty,x]:x\in R\}$$ then $$\mathcal{B}=B(R)=\sigma(\xi)$$
To introduce the concepts of $\Sigma$-measurable function and random variable, we should first define the preimage of a set function.
\begin{definition}[preimage of set function]
	Let $h:\Omega\rightarrow\mathcal{X}\subset\R$ be a set function. Then the preimage of $S\subset\R$ over $h$ is
	$$h^{-1}:=\{w\in\Omega:h(w)\in S \}$$
\end{definition}
Note that the image of the set function is not necessary a really set. That is, it can also be a single point in $\mathbb{R}$ just as the following definition of $\Sigma$-measurable function.
\begin{definition}[$\Sigma$-measurable function]
Suppose that $h:\Omega\rightarrow\R$ then h is called a $\Sigma$-measurable function if $h^{-1}:\mathcal{B}\rightarrow\Sigma$, that is, $h^{-1}(S)\in \Sigma\ \forall S\in\mathcal{B}.$
And we write $m\Sigma$ to be the set of all these measurable function on $\Omega$.
\end{definition} 
\begin{intuition}[measurable function]
	A set function $h$ is $\Sigma$-measurable if the preimages of every Borel set lies in $\Sigma$.
\end{intuition} 
\begin{definition}[Random variable]
Let $(\Omega,\mathcal{A})$ be our sample space and family of events. A random variable X  is an element of $m\mathcal{A}$ which will satisfied our random mechanism P. Thus,
$$X:\Omega\rightarrow\R, X^{-1}:\mathcal{B}\rightarrow\mathcal{A}$$
\end{definition}
\begin{intuition}[Random variable]
Suppose X is a random variable carried by arbitrary probability triple $(\Omega,\mathcal{A},P)$ then we have $$\mathcal{B}\xrightarrow{X^{-1}} \mathcal{A} \xrightarrow{P} [0,1]$$
Define $\mu_x$ by 
$$\mu_x:=P\circ X^{-1} \text{  i.e.  } \mu_x(B)=P(\{w:X(w)\in B\})\  \forall B\in \mathcal{B}$$
And $\mu_x$ is actually a probability measure on $(R,\mathcal{B})$, then for any abstract probability space we can use X to change it to $(R,\mathcal{B})$ which we are familiar with. 
$$(\Omega,\mathcal{A},P)\xrightarrow{X} (R,\mathcal{B},\mu_x)$$ and define the function $F_X:R\rightarrow [0,1]$ as: 
$$F_x(c):=\mu_x((-\infty,c])=P(\{w:X(w)\leq c\})=P(X\leq c)$$
\end{intuition}
Now how can we verify any real-valued function X on $\Omega$ is indeed a random variable? We first begin from the reverse image of X. 
\begin{property}[Properties of set function]\label{propertyofsetfuncs}
	Let $X:\Omega\rightarrow\mathcal{X}\subset\R$ be a set function, then the following properties hold. (Note that here $X$ is not necessary a random variable)
	\begin{enumerate}
		\item (Close under complementation): $X^{-1}(S^C) = (X^{-1}(S))^C$
		\item (Close under union): $X^{-1}(\bigcup_{\alpha\in\Gamma}S_{\alpha}) = \bigcup_{\alpha\in\Gamma}X^{-1}(S_i)$, where $\{S_{\alpha}\}_{\alpha\in\Gamma}$
		\item (Close under intersection): $X^{-1}(\bigcap_{\alpha\in\Gamma}S_{\alpha}) = \bigcap_{\alpha\in\Gamma}X^{-1}(S_i)$, where $\{S_{\alpha}\}_{\alpha\in\Gamma}$
	\end{enumerate}
\end{property}

By definition, a random variable should satisfy the condition that the preimage of every Borel set should in the event space $\mathcal{A}$. However, it's difficult to check since it's hard to enumerate all Borel set and claim the results. Thus, we would like to find a relaxed but necessary condition for a set function to be a random variable. And the following theorem does so.

\begin{theorem}[necessary and sufficient condition for random variable]\label{rvnscondition}
	Let $X:\Omega\rightarrow\mathcal{X}\subset\R$ be a set function, then
	$$X\mbox{ is a random variable}\Leftrightarrow \forall x\in\R,\ \{w:X(w)\leq x\}\in\mathcal{A}$$
\end{theorem}
The proof is left in Appendix~\ref{sec:proofrvnscondition}
\section{From Set Function to Value Function}
Note that random variable is a function that helps us map the event set $\mathcal{A}$ onto the Borel set. The importance here is that know we can do the equivalent operation on {\bf real number} instead of arbitrary $\sigma$-algebra. This not only provides us a general and uniform way to play but also gives us the opportunity to operate on an {\bf ordered} set.

Soon, we might wonder can we play with an even more general function: value function instead of just a set function onto real number? And by the construction of random variable, there are two direct value functions that play an important role in probability theory.
\begin{theorem}[value functions]
	Let $X$ be a random variable w.r.t. $(\Omega,\mathcal{A},P)$, then we can define
	\begin{itemize}
		\item The measure function of $X$ is $\mu_X:\mathcal{B}\rightarrow\mathbf{R}^+$ such that $$\forall B\in\mathcal{B},\ \mu_X(B):=P\{w:X(w)\in B\}$$
		\item The cumulative distribution of $X$ is $F_X:\mathbf{R}\rightarrow\mathbf{R}^+$ such that $$\forall x\in\mathbf{R},\ F_X(x):=P\{w:X(w)\leq x \}$$
	\end{itemize}
\end{theorem}

Here, we also have a necessary and sufficient condition for cumulative function. Intuitively, when a function $F$ satisfies the following conditions, then there's a random variable with its unique cumulative distribution being $F$.

\begin{theorem}[necessary and sufficient condition of cumulative distribution]\label{cdnscondition}
	$F$ is a cumulative distribution iff
	\begin{itemize}
		\item (upper and lower bound) $\lim_{x\rightarrow\infty}F(x)=0,\ \lim_{x\rightarrow\infty}F(x) = 1$
		\item (non-decreasing) $\forall x\leq y$, $F(x)\leq F(y)$
		\item (right continuous) $\lim_{x\rightarrow x_0^+} = F(x_0)$
	\end{itemize}
\end{theorem}
The proof is left in Appendix~\ref{sec:proofcfnscondition}.


\begin{intuition}[set function and value function]
	Careful with the difference of 
	\begin{itemize}
		\item ({\bf random variable}): $X:\Omega\rightarrow S\subset\R$
		\item ({\bf measure function}): $\mu_X:\mathcal{B}\rightarrow\R^+$
		\item ({\bf cumulative distribution}): $F_X:\R\rightarrow\R^+$
	\end{itemize}
\end{intuition}


\section{Proof of the necessary and sufficient condition of random variable}\label{sec:proofrvnscondition}
Recall that Theorem~\ref{rvnscondition} provides a necessary and sufficient condition for a set function to be a random variable. Here, we prove the correctness of the theorem.

($\Rightarrow$) First, we can see that $\{w:X(w)\leq x\} = X^{-1}[(-\infty,x)]$. Thus, it's sufficient to show that $(-\infty,x)$ is in Borel set $\forall x$. And this is simple since $(-\infty,x) = \bigcup_{n\in\mathbb{N}}[-n,x]\in\mathcal{B}$.

($\Leftarrow$) This direction can be proved in two steps:
\begin{enumerate}
	\item Show that $\forall a<b,\ X^{-1}([a,b])\in\mathcal{A}$.
	\item Show that $\xi = \{S:\exists w\in\Omega,\ X(w)=S \}$ is a $\sigma$-algebra.
\end{enumerate}
With these two results, we can see that the image of $X$ contains $\mathcal{B}$. Thus, $X$ is a random variable. The following show the correctness of these two:
\begin{enumerate}
	\item $\forall a<b$, consider
	\begin{align*}
	X^{-1}([a,b]) &= X^{-1}((-\infty,b]\backslash(-\infty,a]) = X^{-1}((-\infty,b]\cap(-\infty,a])^C)\\
	&= X^{-1}((-\infty,b]) \cap X^{-1}((-\infty,a])^C)\\
	&=  X^{-1}((-\infty,b]) \cap X^{-1}((-\infty,a]))^C\in\mathcal{A}
	\end{align*}
	\item Check the three axioms of $\sigma$-algebra:
	\begin{enumerate}
		\item Clearly, $\emptyset\in\xi$.
		\item $\forall S\in\xi$, $X^{-1}(X^C) = X^{-1}(S)^C\in\mathcal{A}$ by Property~\ref{propertyofsetfuncs}. Thus $S^C\in\xi$.
		\item $\forall \{S_{\alpha}\}_{\alpha\in\Gamma}\in\xi$, $X^{-1}(\bigcup_{\alpha\in\Gamma}S_{\alpha}) = \bigcup_{\alpha\in\Gamma}X^{-1}(S_{\alpha})\in\mathcal{A}$. Thus $\bigcup_{\alpha\in\Gamma}S_{\alpha}\in\xi$.
	\end{enumerate}
\end{enumerate}


\section{Proof of the necessary and sufficient condition of cumulative function}\label{sec:proofcfnscondition}

Take $\Omega=(0,1)$, $\mathcal{A}=\mathcal{B}(0,1)$ and P be Lebesgue measure.
\begin{remark}
If $B=(a,b)\text{ or }[a,b] \text{ then } Leb(B)=b-a$, i.e. the measure of an interval is its length.
\end{remark}
For $w\in(0,1)$ define $$X(w)=sup\{y:F(y)<w\}$$
We need to show that X is a random variable and the distribution function $F_x=F$.
We must first verify X(w) is well-defined:
\begin{enumerate}
\item since $w>0$ and $\lim_{x\rightarrow -\infty}F(x)=0$, so $\{y:F(y)<w\}\neq\phi$.
\item since $w<1$ and $\lim_{x\rightarrow \infty}F(x)=1$, so $\{y:F(y)<w\}$ bounded above.
\end{enumerate}
So $X(w)=sup\{y:F(y)<w\}$ exists in $\mathcal{R} \ \forall w\in(0,1)$, X(w) is well defined.We further claim that:
$$\{w:X(w)\leq c\}=\{w:w\leq F(c)\}=(0,F(c)]$$ 
Then X is a random variable since $$\{w:X(w)\leq c\}=(0,F(c)]\in \mathcal{B}(0,1)$$
Aware that $F(c)$ may be 0 or 1 for some $c\in R$, it is not that trivial to show such c will let $(0,F(c)]\in \mathcal{B}$ but since $\{w:X(w)\leq c\}=\{w:w\leq F(c)\}$ and $w\in (0,1)$, we have:
$$F(c)=0,\{w:w\leq F(c)\}=\phi\in\mathcal{B}\text{ and }F(c)=1,\{w:w\leq F(c)\}=\Omega\in\mathcal{B}$$
And $$F_x(c)=P(X^{-1}(\infty,c))=Leb(\{w:X(w)\leq c\})=Leb((0,F(c)])=F(c)$$
So we can define a random variable X on $(\Omega,\mathcal{B}(0,1),Leb)$ which its distribution function is just given cumulative function. We complete the proof by showing:
$$\{w:X(w)\leq c\}\subset\{w:w\leq F(c)\},\{w:X(w)\leq c\}\supset\{w:w\leq F(c)\}$$
First observe that $$F(X(w)+\delta)\geq w,\ \forall \delta>0$$ Since if $F(X(w)+\delta)<w$ it implies a contradiction: $$X(w)+\delta\in\{y:F(y)<w\}\leq X(w)$$ And $$\lim_{\delta\rightarrow 0}F(X(w)+\delta)\xlongequal{\text{right-continuous}}F(X(w))\geq w$$  
\begin{enumerate}
\item $\subset$ case: $X(w)\leq c\implies F(c)\geq F(X(w))\geq w $ (non-decreasing) so, 
 $$\{w:X(w)\leq c\}\subset\{w:w\leq F(c)\}$$
\item $\supset$ case: $F(c)\geq w\implies$c is an upper bound of $\{y:F(y)<w\}, c\geq\sup\{y:F(y)<w\}=X(w)$
 $$\{w:X(w)\leq c\}\supset\{w:w\leq F(c)\}$$
\end{enumerate}

\paragraph{\color{red} Chi-Ning's version}
Here, we are going to show that the three conditions in Theorem~\ref{cdnscondition} is necessary and sufficient for a function $F$ to be a cumulative distribution of a random variable.

(Necessary)
Let $F$ be the cumulative distribution of random variable $X$ defined on $(\Omega,\mathcal{A},P)$. Check the three conditions as follow:
\begin{enumerate}
	\item First, let $A_n \ (-\infty,-n]$. As $A_n\rightarrow\emptyset$ when $n\rightarrow\infty$, by the continuity axiom, $\lim_{n\rightarrow\infty}P(A_n) = \lim_{n\rightarrow\infty}F(-n)=0$. Since, $\lim_{x\rightarrow-\infty}F(x)\leq\lim_{x\rightarrow-\infty}F(\floor*{x})=\lim_{n\rightarrow\infty}F(-n)=0$, we obtain the lower bound. The upper bound can be obtained with the same technique.
	\item If $x\leq y$, then $(-\infty,x]\subseteq(=\infty,y]$. Clearly, $F(x) = P((-\infty,x])\leq P((-\infty,y])=F(y)$.
	\item Let $\{x_n\}$ be a sequence strictly decreases (approaches) to $x_0$, then $A_n:=(-\infty,x_n]\rightarrow A = (-\infty,x]$. That is, $F(x) = P(A)$ and $F(x_n) = P(A_n),\ \forall n$. Now consider $B_n=\R\backslash A_n,\ \forall n$ and $B=\R\backslash A$. For convenience, let $A_0 = \Omega$ and $B_0 = \emptyset$. Then we have
	\begin{align*}
	1-P(A) &= P(B) = P(\lim_{n\rightarrow\infty}B_n) = P(\bigcup_{n=1}^{\infty}B_n\backslash B_{n-1})\\
	&=\sum_{n=1}^{\infty} P(B_n\backslash B_{n-1}) = \sum_{n=1}^{\infty} P(A_{n-1}) - P(A_n)\\
	& = 1-\lim_{n\rightarrow\infty}P(A_n)
	\end{align*}
	Thus, $F(x_0) = P(A) = \lim_{n\rightarrow\infty}P(A_n) = \lim_{x\rightarrow x_0^+}F(x)$.
\end{enumerate}

(sufficient) Let $F$ be a function satisfies the three conditions in Theorem~\ref{cdnscondition}. Take $(\Omega=(0,1),\mathcal{A}=\mathcal{B}(0,1),P = \mathcal{L}(0,1))$ as the probability space. Now, we are going to construct a random variable $X:\Omega\rightarrow\mathcal{X}\subset\R$ w.r.t $F$ over $(\Omega,\mathcal{A},P)$. $X$ is defined as
$$\forall w\in\Omega,\ X(w):=\inf\{x:w\leq F(x) \}$$
To show that $X$ is indeed the random variable with $F$ as the underlying cumulative distribution. We need to first demonstrate that $X$ is a random variable then show the relation between $X$ and $F$.
\begin{enumerate}
	\item By Theorem~\ref{rvnscondition}, it's sufficient to show that $\forall x\in\R$, $X^{-1}((-\infty,x])\in\mathcal{A}$. Consider $\forall x\in\R$,
	\begin{align*}
	X^{-1}((-\infty,x]) &= \{w:X(w)\leq(-\infty,x] \} = \{w:\inf\{y:w\leq F(y)\}\in(-\infty,x]  \}\\
	&=\{w:w\leq F(x) \} = (0,F(x)]\in\mathcal{A}
	\end{align*}
	Thus, $X$ is a random variable over $(\Omega,\mathcal{A},P)$.
	\item Check that $\forall x\in\R$, $F(x) = P(X\leq x)$. Consider $\forall x\in\R$
	\begin{align*}
	P(X\leq x) &= P(\{w:X(w)\leq x \}) = P(\{w:\inf\{y:w\leq F(y)\}\leq x \})\\
	(\because y\leq x\Rightarrow F(y)\leq F(x))&=P(\{w:\inf\{F(y):w\leq F(y)\}\leq F(x) \})\\
	&=P(\{w:w\leq F(x) \}) = P((0,F(x)])\\
	(\because P=\mathcal{L})&=F(x)
	\end{align*}
	That is, $F$ is the cumulative distribution of random variable $X$.
\end{enumerate}

\end{document}




