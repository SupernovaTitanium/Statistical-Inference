\documentclass[../Transformation.tex]{subfiles}
\begin{document}
	\begin{center}
		\renewcommand{\arraystretch}{2}
		\begin{bfseries}
			\begin{tabular}{|c|}
				\hline
				Statistical Inference I \hfill Prof. Chin-Tsang Chiang\\
				\hspace{15em} {\large Lecture Notes 10} \hspace{15em}\ \\
				\lecdate \hfill Scribe: \scribe\\
				\hline
			\end{tabular}
			\renewcommand{\arraystretch}{1}
		\end{bfseries}
	\end{center}
	
\begin{lemma}[inversion theorem]
	Let $X$ be a random variable with characteristic function $\phi_X(t)$ and $a,b\in\R$ with $a<b$, then
	\begin{enumerate}
		\item For any random variable $X$,
		$$P(a<X<b) + \frac{1}{2}P(X=a) + \frac{1}{2}P(X=b) = \limT\frac{1}{2\pi}\int_{-T}^{T}\frac{e^{-ita}-e^{-itb}}{it}\phi_X(t)dt$$
		\item If $X$ is a continuous random variable, then
		$$f_X(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty}e^{-itx}\phi_X(t)dt\ \ a.e.$$
	\end{enumerate}
\end{lemma}
\section{Characteristic function}
\begin{intuition}[inversion theorem]
	Inversion theorem provides a {\it isomorphism} between {\bf distribution} and {\bf characteristic function}.
\end{intuition}

\begin{proof}
	The proof is divided into four steps. The first step introduce a integration tool for latter usage. The second step provides a clean form of the original term. The third step uses dominant theorem to show the convergence. The last step gives the density function.
	\begin{enumerate}
		\item {\bf claim}: $\int_0^{\infty}\frac{\sin\alpha x}{x}dx = \frac{\pi}{2}\mbox{sign}(\alpha)$\\
		\begin{align*}
		\int_0^{\infty}\frac{\sin\alpha x}{x}dx &= \int_0^{\infty}\int_0^{\infty}\sin\alpha x\ e^{-ux}dudx\\
		(\mbox{by Fubini's thm})&=\int_0^{\infty}\int_0^{\infty}\sin\alpha x\ e^{-ux}dxdu\\
		&= ...\mbox{ some change of integrals }...\\
		&=\frac{\pi}{2}\mbox{sign}(\alpha)
		\end{align*}
		\item Consider
		\begin{align*}
		\frac{1}{2\pi}\int_{-T}^{T}\frac{e^{-ita}-e^{-itb}}{it}\phi_X(t)dt &= \frac{1}{2\pi}\int_{-T}^{T}\int_{-\infty}^{\infty}\frac{e^{-ita}-e^{-itb}}{it}e^{itx}dF_X(x)dt\\
		&=\frac{1}{2\pi}\int_{-T}^{T}\int_{-\infty}^{\infty}\frac{\cos t(x-a)+i\sin t(x-a)}{it}dF_X(x)dt\\
		&=\frac{1}{2\pi}\int_{-T}^{T}\int_{-\infty}^{\infty}\frac{\cos t(x-b)+i\sin t(x-b)}{it}dF_X(x)dt\\
		(\because\mbox{symmetry})&=\frac{1}{\pi}\int_0^{T}\int_{-\infty}^{\infty}\frac{\sin t(x-a) - \sin t(x-b)}{t}dF_X(x)dt\\
		(\mbox{by Fubini's thm})&=\frac{1}{\pi}\int_{-\infty}^{\infty}\int_0^{T}\frac{\sin t(x-a) - \sin t(x-b)}{t}dtdF_X(x)
		\end{align*}
		\item By dominant convergence theorem,
		\begin{align*}
		\frac{1}{2\pi}\int_{-T}^{T}\frac{e^{-ita}-e^{-itb}}{it}\phi_X(t)dt &\xrightarrow{T\rightarrow\infty}\frac{1}{\pi}\int_{-\infty}^{\infty}\int_0^{\infty}\frac{\sin t(x-a) - \sin t(x-b)}{t}dtdF_X(x)\\
		&=\frac{1}{\pi}\int_{x\in(-\infty,a)}\int_0^{T}\frac{\sin t(x-a) - \sin t(x-b)}{t}dtdF_X(x)\\
		&+\frac{1}{\pi}\int_{x=a}\int_0^{T}\frac{\sin t(x-a) - \sin t(x-b)}{t}dtdF_X(x)\\
		&+\frac{1}{\pi}\int_{x\in(a,b)}\int_0^{T}\frac{\sin t(x-a) - \sin t(x-b)}{t}dtdF_X(x)\\
		&+\frac{1}{\pi}\int_{x=b}\int_0^{T}\frac{\sin t(x-a) - \sin t(x-b)}{t}dtdF_X(x)\\
		&+\frac{1}{\pi}\int_{x\in(b,\infty)}\int_0^{T}\frac{\sin t(x-a) - \sin t(x-b)}{t}dtdF_X(x)\\
		(\mbox{by the tool in 1.})&=0+\frac{1}{2}P(X=a) + P(a<X<b) + \frac{1}{2}P(X=b) + 0
		\end{align*}
	\item Suppose X is continuous and $\int_{\mathcal{R}}|\phi_X(t)dt|<\infty$,
	\begin{align*}
	\int_a^bf(x)dx&=\frac{1}{2\pi}\int_a^b\int_{\mathcal{R}}e^{-itx}\phi_X(t)dtdx\\
	&=\frac{1}{2\pi}\int_{\mathcal{R}}(\int_a^be^{-itx}dx)\phi_X(t)dt\\
	&=\frac{1}{2\pi}\int_{\mathcal{R}}\frac{e^{-ita}-e^{-itb}}{it}\phi_X(t)dt\\
	&=\lim_{T\rightarrow\infty}\frac{1}{2\pi}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\phi_X(t)dt\\
	&=\frac{1}{2}P(X=a) + P(a<X<b) + \frac{1}{2}P(X=b)\\
	&=P(a<X<b)
	(  \mbox{ further set b=x, a$\downarrow -\infty$})
	\end{align*}
	\end{enumerate}
\end{proof}
\begin{corollary}
For probability measures $\mu_X$ and $\mu_Y$ on $\mathcal{B}(\mathcal{R})$, the equality $\phi_{\mu_X}=\phi_{\mu_Y}$ implies that $\mu_X=\mu_Y$.
\end{corollary}
\begin{proof}
From inversion theorem, we have $\mu_X((a,b))=\mu_Y((a,b))$ $\forall a,b\in C$, where C is the set of all $z\in\mathcal{R}$ such that $\mu_X(\{z\})=\mu_Y(\{z\})=0$. Since $C^c$ is at most countable. The family of $\{(a,b):a.b\in C\}$ of intervals is a $\pi$-system generating $\mathcal{B}(\mathcal{R})$. $\mu_X$ and $\mu_Y$ agrees on a $\pi$-system also agrees on the $\sigma$-algebra generated by it.
\end{proof}
\section{Convergence}
\begin{theorem}\label{thm:convergenceincharfunc}
	Let $\{X_n \}$ be a sequence of random  variables with characteristic functions $\phi_{X_n}(t)$. Suppose that
	\begin{itemize}
		\item $\limn\phi_{X_n}(t) = \phi_X(t)$ for $t$ in a neighborhood of 0. (pairwise convergence)
		\item $\phi_X(t)$ is a characteristic function of some random variable $X$.
	\end{itemize}
	Then,
	$$\limn F_{X_n}(x) = F_X(x),\ \forall x\mbox{ such that $F_X(x)\mbox{ is continuous (weakly convergence)}$}$$
\end{theorem}
	\begin{proof}
	Let a and b be continuous points of $F_X(x)$ and $F_{X_n}(x)$ for $n\geq N_0$ for some $n\in\mathcal{N}$
	\begin{align*}
	F_X(b)-F_X(a)&=\lim_{T\rightarrow\infty}\frac{1}{2\pi}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\phi_X(t)dt\\
	&=\lim_{T\rightarrow\infty}\frac{1}{2\pi}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\lim_{n\rightarrow\infty}\phi_{X_n}(t)dt\\
	&=\lim_{n\rightarrow\infty}\lim_{T\rightarrow\infty}\frac{1}{2\pi}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\phi_{X_n}(t)dt\ \mbox{(since $|\phi_{X_n}(t)|\leq1$, dominated)}\\
	&=\lim_{n\rightarrow\infty}(F_{X_n}(b)-F_{X_n}(a))
	\end{align*}
	By setting b=x, a$\downarrow-\infty$ one obtains $\limn F_{X_n}(x) = F_X(x)$.
	\end{proof}
{\bf Remark}: We don't have to worry about the discrete points since they must converge to the right value.\\
\begin{intuition}[convergence of characteristic function]
	Theorem~\ref{thm:convergenceincharfunc} tells us that if r.v.s converge in characteristic functions, then r.v.s also converge in distribution.
\end{intuition}
\noindent{\bf Remark}: Tips for calculating MGF: consider the MGF of binomial distributed random variable $X$ such that $f_X(x)={n\choose x}p^{x}(1-p)^{n-x}\mathbf{1}_{0,1,...,n}$. We have
\begin{align*}
M_{X_n}(t) &= \sum_{x=0}^n{n\choose x}p^{x}(1-p)^{n-x}e^{tx}\\
&=\sum_{x=0}^n{n\choose x}(pe^t)^x(1-p)^{n-x}\\
&=\sum_{x=0}^n{n\choose x}\frac{(pe^t)^x(1-p)^{n-x}}{[(pe^t)+(1-p)]^n}[(pe^t)+(1-p)]^n\\
(\mbox{set }p'=\frac{pe^t}{pe^t+(1-p)})&=\sum_{x=0}^n{n\choose x}p'^x(1-p')^{n-x}[(pe^t)+(1-p)]^n\\
&=[(pe^t)+(1-p)]^n\sum_{x=0}^n{n\choose x}p'^x(1-p')^{n-x}\\
&=[(pe^t)+(1-p)]^n
\end{align*}

\end{document}
